from sentenceSegmentation import SentenceSegmentation
from tokenization import Tokenization
from inflectionReduction import InflectionReduction
from stopwordRemoval import StopwordRemoval
from informationRetrieval import LSA
from evaluation import Evaluation
import random
import time
from ESA import ESA
# import wikipedia
import pandas as pd
from sys import version_info
import argparse
import json
import matplotlib.pyplot as plt

# Input compatibility for Python 2 and Python 3
if version_info.major == 3:
    pass
elif version_info.major == 2:
    try:
        input = raw_input
    except NameError:
        pass
else:
    print("Unknown python version - input function not safe")


class SearchEngine:

    def __init__(self, args):
        self.args = args

        self.tokenizer = Tokenization()
        self.sentenceSegmenter = SentenceSegmentation()
        self.inflectionReducer = InflectionReduction()
        self.stopwordRemover = StopwordRemoval()

        self.evaluator = Evaluation()

        self.latentSemanticAnalysis = LSA()
        self.explicitSemanticAnalysis = ESA()

    def segmentSentences(self, text):
        """
        Call the required sentence segmenter
        """
        if self.args.segmenter == "naive":
            return self.sentenceSegmenter.naive(text)
        elif self.args.segmenter == "punkt":
            return self.sentenceSegmenter.punkt(text)

    def tokenize(self, text):
        """
        Call the required tokenizer
        """
        if self.args.tokenizer == "naive":
            return self.tokenizer.naive(text)
        elif self.args.tokenizer == "ptb":
            return self.tokenizer.pennTreeBank(text)

    def reduceInflection(self, text):
        """
        Call the required stemmer/lemmatizer
        """
        return self.inflectionReducer.reduce(text)

    def removeStopwords(self, text, document_type):
        """
        Call the required stopword remover
        """
        return self.stopwordRemover.fromList(text, document_type)

    def preprocessQueries(self, queries, document_type):
        """
        Preprocess the queries - segment, tokenize, stem/lemmatize and remove stopwords
        """

        # Segment queries
        segmentedQueries = []
        for query in queries:
            segmentedQuery = self.segmentSentences(query)
            segmentedQueries.append(segmentedQuery)
        json.dump(segmentedQueries, open(self.args.out_folder + "segmented_queries.txt", 'w'))
        # Tokenize queries
        tokenizedQueries = []
        for query in segmentedQueries:
            tokenizedQuery = self.tokenize(query)
            tokenizedQueries.append(tokenizedQuery)
        json.dump(tokenizedQueries, open(self.args.out_folder + "tokenized_queries.txt", 'w'))
        # Stem/Lemmatize queries
        reducedQueries = []
        for query in tokenizedQueries:
            reducedQuery = self.reduceInflection(query)
            reducedQueries.append(reducedQuery)
        json.dump(reducedQueries, open(self.args.out_folder + "reduced_queries.txt", 'w'))
        # Remove stopwords from queries
        stopwordRemovedQueries = []
        for query in reducedQueries:
            stopwordRemovedQuery = self.removeStopwords(query, document_type)
            stopwordRemovedQueries.append(stopwordRemovedQuery)
        json.dump(stopwordRemovedQueries, open(self.args.out_folder + "stopword_removed_queries.txt", 'w'))

        preprocessedQueries = stopwordRemovedQueries
        return preprocessedQueries

    def preprocessDocs(self, docs, document_type):
        """
        Preprocess the documents
        """

        # Segment docs
        segmentedDocs = []
        for doc in docs:
            segmentedDoc = self.segmentSentences(doc)
            segmentedDocs.append(segmentedDoc)
        json.dump(segmentedDocs, open(self.args.out_folder + "segmented_docs.txt", 'w'))
        # Tokenize docs
        tokenizedDocs = []
        for doc in segmentedDocs:
            tokenizedDoc = self.tokenize(doc)
            tokenizedDocs.append(tokenizedDoc)
        json.dump(tokenizedDocs, open(self.args.out_folder + "tokenized_docs.txt", 'w'))
        # Stem/Lemmatize docs
        reducedDocs = []
        for doc in tokenizedDocs:
            reducedDoc = self.reduceInflection(doc)
            reducedDocs.append(reducedDoc)
        json.dump(reducedDocs, open(self.args.out_folder + "reduced_docs.txt", 'w'))
        # Remove stopwords from docs
        stopwordRemovedDocs = []
        for doc in reducedDocs:
            stopwordRemovedDoc = self.removeStopwords(doc, document_type)
            stopwordRemovedDocs.append(stopwordRemovedDoc)
        json.dump(stopwordRemovedDocs, open(self.args.out_folder + "stopword_removed_docs.txt", 'w'))

        preprocessedDocs = stopwordRemovedDocs
        return preprocessedDocs

    def handleDatasetLSA(self):
        """
        - preprocesses the queries and documents, stores in output folder
        - invokes the IR system
        - evaluates precision, recall, fscore, nDCG and MAP
          for all queries in the Cranfield dataset
        - produces graphs of the evaluation metrics in the output folder
        """

        # Read queries
        queries_json = json.load(open(args.dataset + "cran_queries.json", 'r'))[:]
        query_ids, queries = [item["query number"] for item in queries_json], \
                             [item["query"] for item in queries_json]
        # Process queries
        processedQueries = self.preprocessQueries(queries, 'docs')

        # Read documents
        docs_json = json.load(open(args.dataset + "cran_docs.json", 'r'))[:]
        doc_ids, docs = [item["id"] for item in docs_json], \
                        [item["body"] for item in docs_json]
        # Process documents
        processedDocs = self.preprocessDocs(docs, 'docs')

        # Build document index
        X = self.latentSemanticAnalysis.buildIndex(processedDocs, processedQueries)
        rank = 250
        doc_IDs_ordered = self.latentSemanticAnalysis.rank(X,
                                                           len(processedDocs), len(processedQueries), rank)

        # Read relevance judements
        qrels = json.load(open(args.dataset + "cran_qrels.json", 'r'))[:]

        # Calculate precision, recall, f-score, MAP and nDCG for k = 1 to 10
        precisions, recalls, fscores, MAPs, nDCGs = [], [], [], [], []
        for k in range(1, 11):
            precision = self.evaluator.meanPrecision(
                doc_IDs_ordered, query_ids, qrels, k)
            precisions.append(precision)

            recall = self.evaluator.meanRecall(
                doc_IDs_ordered, query_ids, qrels, k)
            recalls.append(recall)

            fscore = self.evaluator.meanFscore(
                doc_IDs_ordered, query_ids, qrels, k)
            fscores.append(fscore)

            print("Precision, Recall and F-score @ " +
                  str(k) + " : " + str(precision) + ", " + str(recall) +
                  ", " + str(fscore))
            MAP = self.evaluator.meanAveragePrecision(
                doc_IDs_ordered, query_ids, qrels, k)
            MAPs.append(MAP)
            nDCG = self.evaluator.meanNDCG(
                doc_IDs_ordered, query_ids, qrels, k)
            nDCGs.append(nDCG)
            print("MAP, nDCG @ " +
                  str(k) + " : " + str(MAP) + ", " + str(nDCG))

        # Plot the metrics and save plot
        plt.plot(range(1, 11), precisions, label="Precision")
        plt.plot(range(1, 11), recalls, label="Recall")
        plt.plot(range(1, 11), fscores, label="F-Score")
        plt.plot(range(1, 11), MAPs, label="MAP")
        plt.plot(range(1, 11), nDCGs, label="nDCG")
        plt.legend()
        plt.title("LSA - Cranfield Dataset")
        plt.xlabel("k")
        plt.savefig(args.out_folder + "eval_plot_LSA.png")

        '''
        # Code for finding the best rank for SVD matrix.


        time1 = time.time()
        l = []

        qrels = json.load(open(args.dataset + "cran_qrels.json", 'r'))[:]
        for rank in range(100, 1390, 10):

            doc_IDs_ordered = self.latentSemanticAnalysis.rank(X,
                                                           len(processedDocs), len(processedQueries), rank)

            # Read relevance judements

            # Calculate precision, recall, f-score, MAP and nDCG for k = 1 to 10
            precisions, recalls, fscores, MAPs, nDCGs = [], [], [], [], []
            for k in range(1, 11):
                nDCG = self.evaluator.meanNDCG(
                    doc_IDs_ordered, query_ids, qrels, k)
                nDCGs.append(nDCG)
            maxNDCG = max(nDCGs)

            print('Maximum nDCG = {} at {}'.format(maxNDCG, rank))
            l.append([maxNDCG, rank])
            mx = max(l)
            print("Max Score till now = {} at rank = {}".format(mx[0], mx[1]))
            time2 = time.time()
            print('Time = ', (time2 - time1) // 60, "mins", (time2 - time1) % 60, "secs")

        '''

    def handleDatasetESA(self):
        """
                - preprocesses the queries and documents, stores in output folder
                - invokes the IR system
                - evaluates precision, recall, fscore, nDCG and MAP
                  for all queries in the Cranfield dataset
                - produces graphs of the evaluation metrics in the output folder
                """

        # Read queries

        '''
        This is code to extract all 21981 wikipedia articles based on domain
        and properly clean the data and store it in a json file to save compute time.
        Kept here only for examining purpose.


        Read documents
        docs_json = json.load(open(args.dataset + "cran_docs.json", 'r'))[:]
        title_ids, titles = [item["id"] for item in docs_json], \
                        [item["title"] for item in docs_json]

        # Process documents
        vocabTitles = set()
        processedTitles = self.preprocessDocs(titles, 'docs')

        for docs in processedTitles:
            for word in docs:
                vocabTitles.add(word)
        print(len(vocabTitles))



        vocabBody = set()
        for docs in processedTitles:
            for word in docs:
                vocabBody.add(word)
        print(len(vocabBody))


        wordsToSearch = sorted(list(vocabBody.difference(vocabTitles)))


        print(len(wordsToSearch))
        print(wordsToSearch)

        wikipediaArticles = []
        wikipedia.set_lang("en")
        count = 0
        checkCount = 0
        for word in wordsToSearch:
            print("check count = ", checkCount)
            print('word = ', word)
            searched_pages = wikipedia.search(word, results=4)
            for page in searched_pages:
                try:
                    obj = wikipedia.summary(page)
                    wikipediaArticles.append({
                        'id': count+1,
                        'title': page,
                        'body': obj
                    })

                except:
                    pass
                count+=1
            checkCount += 1
        print(len(wikipediaArticles))

        with open('wikipediaArticlesNew.json', 'w') as outfile:
            json.dump(wikipediaArticles, outfile)

        time1 = time.time()

        docs_json = json.load(open("wikipedia1.json", 'r'))[:]
        article1 = [item["body"] for item in docs_json]
        processedArticles1 = self.preprocessDocs(article1, 'articles')


        docs_json = json.load(open("wikipedia2.json", 'r'))[:]
        article2 = [item["body"] for item in docs_json]
        processedArticles2 = self.preprocessDocs(article2, 'articles')


        docs_json = json.load(open("wikipedia3.json", 'r'))[:]
        article3 = [item["body"] for item in docs_json]
        processedArticles3 = self.preprocessDocs(article3, 'articles')


        docs_json = json.load(open("wikipedia4.json", 'r'))[:]
        article4 = [item["body"] for item in docs_json]
        processedArticles4 = self.preprocessDocs(article4, 'articles')


        docs_json = json.load(open("wikipedia5.json", 'r'))[:]
        article5 = [item["body"] for item in docs_json]
        processedArticles5 = self.preprocessDocs(article5, 'articles')


        docs_json = json.load(open("wikipedia6.json", 'r'))[:]
        article6 = [item["body"] for item in docs_json]
        processedArticles6 = self.preprocessDocs(article6, 'articles')

        time2 = time.time()

        allArticles = processedArticles1 + processedArticles2 + processedArticles3 + \
                        processedArticles4 + processedArticles5 + processedArticles6

        time3 = time.time()
        print("Time to proccess all articles = ",time2 - time1)
        print("Time to merge all proccessed docs = ", time3 - time2)

        print(len(allArticles))

        s = set()
        numArtices = len(allArticles)
        for i in range(numArtices):
            for j in allArticles[i]:
                s.add(j)


        print("Vocab of all articles = ", len(s))

        alphabetsToRemove = {'ç¦', 'Äª', 'å³¨', 'ä¹ ', 'Ì©', '|', 'á¼”', 'Î—', 'B', 'á¼°', 'åˆ', 'è¦–', 'á¼·',
                             'ã™', 'â€²', 'ÙŒ', 'ç•Œ', 'Ğ²', 'å¤§', 'Ü', 'ï¼ˆ', 'á¼ ', 'É¢', 'ç£', 'Ã¬', 'å°',
                             'Õ¶', 'Õ¢', 'á¼¥', 'æ›¸', 'ğ¤®', 'a', 'p', 'â€', 'å—', 'å¯', 'â€’', 'å‰', 'æ‹¾',
                             'Å‹', 'Î»', 'åµ', 'â€“', 'Ú©', 'ìˆœ', '×œ', 'Ê‰', 'å…¸', 'ë„', 'á»™', 'äºŒ', 'h',
                             'Å¥', 'æˆ’', 'å¾€', 'é·„', 'å¥', 'Ü¡', 'Ğ’', 'à¦Ÿ', 's', 'Îº', 'è€»', 'Ä', 'ã‚¾',
                             'ã‚¶', 'èˆ', 'ç¶œ', '×', 'Ñ‘', 'Ñ', 'äºº', 'áŸ‚', 'Ø§', 'áƒš', 'Û’', 'Å¡', 'È™',
                             'æ¥', 'æ£‹', 'Ö·', 'çš®', 'Ë¨', 'æ­Œ', 'æˆ°', 'æ°—', 'Ğ»', 'Ã²', 'R', 'ì‹œ', 'ç„¡',
                             'å»·', 'å§‹', 'ãƒ™', 'æ¥­', 'èª ', 'É”', 'Éµ', 'Õ¿', 'Ü“', 'áƒ¡', 'æµ·', '^', 'æ˜Š',
                             'å¥³', 'è™½', 'ã', 'ã‚', 'å³¶', 'ï¼´', 'å®¶', 'É¾', 'ç‰©', 'Å½', 'å®…', 'àµ€',
                             'æˆ˜', 'Ì¹', 'ä¸–', 'ë°–', 'à¸', 'á€¸', 'â€¦', 'Î¸', 'é¾', 'æ–¹', 'Ö”', 'Ã§', 'æ§˜',
                             'Ë ', 'ÄŒ', 'ç‰', 'Õ', 'äº‰', 'é™³', 'Ã¯', 'æ²¹', '×˜', 'Ã ', 'â€¡', 'æ„›', 'ä¹‰',
                             'à¤Ÿ', 'åŒ', 'Å¬', 'è¡—', 'Ğ°', 'ã‚µ', 'ã‚', 'Ê‹', 'è‰²', '×›', 'á½€', 'ç›®', 'Î¡',
                             'É¥', 'å¥¶', 'Ğº', 'Î ', 'Ñ…', 'Ğ±', 'ã', '×“', 'æ•…', 'á¾¶', 'Ë¤', 'ç¾©', 'Ö¼',
                             'ç«', 'Ã¾', 'Ç«', '×”', 'ä½“', 'Ù†', 'é‡‡', 'ãƒ¥', 'â‰ ', 'H', 'Ã', 'å²', 'å±±',
                             'ÜŸ', 'â‹…', 'à®Ÿ', 'Ù¾', 'Î¥', '×´', 'åŸ', 'ìˆ', ' ', 'ä»½', 'Å­', 'Õº', 'Ğ¤',
                             'ë§¨', 'ãƒ ', 'áŒ½', 'ê¸', 'á¸', 'Ä•', 'á¼€', 'ë‚¨', 'å°½', 'ì´', 'ä¹‹', 'ì•„',
                             'à¤¾', 'â™¯', 'Ü¬', 'æ˜', '×©', 'ï½', 'áŸ‹', 'É¡', 'å£“', 'æ‰‹', 'á¹‡', 'í”„', 'æ¥µ',
                             '×‘', 'å››', 'âŸ¨', 'å†³', 'á¹¢', 'æ£®', 'è¯º', 'ğ¤£', 'à¤†', 'Ê»', 'ç›¾', 'åº', 'Õ½',
                             'Ù‚', '×ª', 'ë°”', 'è®¸', 'à¤¸', 'Ö´', 'Äƒ', 'Ã´', 'Ñ€', 'Ï‚', 'ìœ¤', 'á“‚', 'r', 'Ğ',
                             'æ¹¯', 'ì˜', 'è„³', 'ë‹ˆ', 'Ã¥', 'çš‡', 'çµ¶', 'ï¼¡', 'Î·', 'å¥½', 'ãƒŠ', 'Ğ½', 'ã¿',
                             'Â·', 'ç¤¾', 'Ü«', 'á½¶', 'æ’ƒ', 'è¡“', 'ç¨®', 'ã‚¬', 'Ä­', 'ã‚½', 'è£', 'á½', 'Ê”',
                             'Øª', 'ã‚³', 'á¼ˆ', 'ì†Œ', 'à´¹', 'É­', 'ç¾', 'Ì ', 'á¼¹', 'çˆ­', 'á¸', 'æ—¥', 'Ç§',
                             'É¨', 'ãƒ¡', 'â„˜', 'í•œ', 'æ”¹', 'â‚‚', 'ï»Ÿ', '× ', 'á¹ƒ', 'àµ‹', 'á‘²', 'á¼¸', 'å°',
                             'ãƒ', 'g', 'Ğµ', 'à¦­', 'Ã¤', 'ï¼‰', 'ã®', 'á½‘', 'Î®', 'Ï€', 'ï¼®', 'ì§€', 'èˆ°',
                             'ç³', 'à¹€', '×™', 'ç‹', 'ä¸ª', 'íŠ¸', 'Ã°', 'á½º', 'á¸—', 'æ', 'éœ²', 'ã', 'Ó©',
                             'â²“', 'ä»”', 'à¸', 'ãƒ»', 'ì˜¤', 'ã¯', 'Ø³', 'ã‚º', 'á€­', 'æµª', 'Ñ‚', 'à¥€', 'à®®',
                             'á¼', 'Ù¹', 'å…¨', 'ä¸œ', 'à¤‚', 'å¤©', 'å¤', 'å£«', '×¦', 'ë§ˆ', 'Ãµ', 'ãŠ', 'ë¶„',
                             'â²¥', 'è®°', 'áƒ›', 'æ´»', 'áˆ˜', 'x', 'æ±¶', 'à¤ª', 'n', 'ç©º', 'ÑŸ', 'æ¡', 'Ì¥',
                             'èª¿', 'Ğ¯', 'çµ', 'å‡±', 'áº½', 'Öµ', 'ç¥–', 'à¤¨', 'è¡›', 'Ñˆ', 'ë²•', 'Ö¹', 'à¸£',
                             'é›™', 'Ø©', 'ãŸ', 'å›½', 'å²›', 'è—©', 'ã‹', 'à³†', 'ïº“', 'æ¸…', 'â€œ', 'Ü¨',
                             '\u200c', 'å¸', 'Õ¯', 'Î¾', 'å‚', 'å½­', 'ê³„', 'á‹°', 'ÅŒ', 'æˆ¦', 'èˆª', 'Ğ”',
                             'ç¥', 'ë“€', 'ãƒ¯', 'æµ©', 'ê»˜', 'Ñ‡', 'á€º', 'ë˜', 'Ä“', 'çŸ›', 'à¸§', 'á', 'ê¸°',
                             'é£›', 'k', 'Çµ', 'à¸„', 'ğ¤±', 'è»½', 'á±', 'ã‚»', 'Ø¨', '×Ÿ', 'ë¥´', 'à²—', 'â€³',
                             'ê¦›', 'â½', 'Î­', '+', 'Õ»', 'ã˜', 'Ùˆ', 'å»¸', 'å¸«', 'Ğ', 'â¦µ', 'w', 'æ©Ÿ',
                             'Ø£', '\u200b', 'ãƒª', 'â¤', 'ë‹µ', 'ìŠˆ', 'é ­', 'à¤·', 'á¹¯', 'ã«', 'æƒ³', 'É›',
                             'å’Œ', 'Q', 'ã‚·', '×', 'â²±', 'É´', 'Ä', 'Û•', 'è¡Œ', 'Ö«', 'æ€', 'æŒ¯', 'ì „',
                             'á¼±', 'ãƒ‰', 'Å ', 'Ã†', 'Å„', 'á¸¥', '×š', 'æ–—', 'Å', 'å', 'æ³•', 'Å', 'å°‘',
                             'é¦™', 'æ†²', 'ç†', 'Ü¥', 'ç®¡', 'Ã¸', 'å§«', 'å‹‡', 'Ä©', 'åŸº', 'í´', 'æ—',
                             'Ã¦', 'Ä', 'ë¼', 'ãƒ„', 'é¬¥', 'é’Ÿ', 'â™¥', 'j', 'ãƒš', 'é¥', 'æ”¾', 'å¸¸', 'Õ¡',
                             'æº–', 'á ¸', 'â²‘', 'Î’', 'Ì', 'æ— ', 'Ãœ', 'ç´š', 'ã‚¯', 'ãƒ¼', 'ê¦•', 'ÄŸ', 'ğŸ¤–',
                             'Êˆ', 'â€¢', 'å°„', 'á¸Œ', 'A', 'à¤¦', 'åŒ–', 'ë…€', 'à½¼', 'à®°', 'è©±', 'Ğ¹', 'å¼',
                             'á¼¦', 'ä¹…', 'Ç”', 'æ ª', 'Ğ¿', 'ë‹¤', 'È', 'å‡†', '\u2060', 'Î¦', 'áˆ­', 'Î¹',
                             'E', 'í‹°', 'é™', 'å·¨', 'Ã«', 'ãˆ', 'á½•', 'Í¡', 'Ø´', 'á ¤', 'ç¢§', 'ìŠ¤', 'é’',
                             'Ìƒ', 'áº“', 'å“', 'â€”', 'ã€‡', 'Ñ—', 'Î¶', 'ãƒˆ', 'à®¿', 'æ‰¹', 'Ğ¦', 'Ãº', 'Õ„',
                             'à½–', 'Ù', 'ä¼', 'à¦¤', 'åœ‹', 'é³³', 'Å', '×¡', 'Î²', 'åº', 'ä»Š', 'ã‚“', 'Z',
                             'éš±', 'ËŒ', 'Õ¹', 'Ñ›', 'Ø¶', 'Ê’', 'Ñº', 'Î±', 'ïº', 'êµ­', 'á»', 'å‰', 'Ø¢',
                             'é¶¯', 'å€¾', 'ä½ ', 'åº·', 'Ã£', 'í‹±', 'ãƒ³', 'ä»¬', 'à¸«', 'Y', 'É½', 'É™', 'Ì',
                             'Ë§', 'Ñ–', 'áš', 'Üª', 'Ø¸', 'çˆ', 'i', 'Ì€', 'á–…', 'Ø²', 'à®ª', 'é®', 'å‰¯', 'âˆ’',
                             'à¯ˆ', 'ê³¼', 'ç²¾', 'åˆ†', 'ğ¤¶', 'Ö¸', 'Î¯', 'â€‘', 'è‡ª', 'É£', 'Åº', 'æ¹˜', 'à´ª',
                             'éšŠ', 'ãƒ†', 'å®‰', 'áŸ†', 'â²', 'á¹£', 'å‹•', 'ïºª', 'É¬', 'ä¸', 'ã¾', 'áƒ˜', 'Ö‚',
                             'ê§€', 'É•', 'Î•', 'Ö½', 'ì—†', 'ã€Š', 'â€', 'é‹', '×•', 'æ…', 'Ü¢', 'à¹ˆ', 'ãƒ',
                             'æœƒ', 'å¥ˆ', 'à¤“', 'Ä™', 'Øµ', 'å…«', 'Ñ', 'à¤', '×', 'Ç·', 'áŸ‰', 'é­”', '\u202c',
                             'Ü ', 'é—˜', 'á½–', 'á ¯', 'ç¤º', 'Ê¾', 'à®¾', 'ä¼š', 'â²™', 'æŒ‡', 'æƒ…', 'ã‚‰', 'è³‡',
                             'å°±', 'ì‘', 'ç´°', 'ç‹', 'é¨', 'áˆ', 'è¢', '×¨', 'à§‹', 'æœˆ', 'è‰¦', 'à¸¡',
                             'ì—­', 'å¯›', 'á¿¶', 'å®—', 'ëª¨', 'éƒ­', 'Î¬', 'é«˜', 'ã‚¹', 'é—œ', 'è™', 'Ãª',
                             'èˆˆ', 'ëœ»', 'Ê±', 'à¸™', 'æœ', 'è‚²', 'ã›', 'ğ¤³', 'Õ‰', 'Å‚', 'Å»', 'ã†', 'Ë€',
                             'æ¾¤', 'æ±‰', 'ï¼Œ', 'Ğ“', 'å‹', 'à¹Š', 'Ö€', 'Ã¢', '_', 'ãµ', 'áˆ„', 'Öš', 'Æ°',
                             'ì£¼', 'ê¦©', 'åµ‹', 'ã‚¸', 'à¥', 'é–“', 'Ã©', 'ê¦¥', 'Ä«', 'Åˆ', 'æŸ»', 'Ğ§', 'åˆº',
                             'Õ´', 'æ‘', 'ãƒ‘', 'Ğ ', 'Ê‚', '×’', '~', 'á¿·', 'ã‚ˆ', 'Ä±', 'Ï†', 'à¸²', 'ÉŸ', '×',
                             'É³', 'Ö°', 'Ùƒ', 'å¦‚', 'â²—', 'á€', 'à¤—', 'Ä¼', 'ÌŒ', 'ê¸‰', 'æ¯’', 'Ê', '×‚', 'Ù…',
                             'å¿«', 'â²', 'ä¿º', 'å¤±', 'å„‚', 'Î›', 'å¯†', 'Ã', 'Ì‘', 'ë¬¸', 'ïº ', 'ã€‹', 'æ¨‘',
                             'ç”±', 'à¤™', 'ä¸‰', 'ã€œ', 'Æ', 'ÑŒ', 'ç·', 'Ñ„', 'á½¸', 'Ä‘', 'Ğ¨', 'ê²Œ', 'Ê·',
                             'Õ', 'ãš', 'ã¨', 'çœ', 'ç®€', 'Ø¡', 'áƒ’', 'ã³', 'éº', 'à´®', 'Ğˆ', 'V', 'ê¦’',
                             'ä¸»', 'O', 'Ê', 'Ù‘', 'é™¢', 'å¯¹', 'ãƒ”', 'Ğ¥', 'Ã·', 'Ö…', 'á–', 'å¤ª', 'â€™',
                             'ã—', 'ãƒŸ', 'å­—', 'à¤²', 'à®•', 'Ñš', 'ã‚­', 'ä½¿', 'í¼', 'åš´', 'ï¸', 'Î½', 'Ã®',
                             'Âµ', 'è´«', 'á¹­', 'ë§Œ', 'á¼•', 'æ¨©', 'ì¼', 'ãƒ–', 'è›‹', 'é‡Œ', 'å³ ', 'Å™', 'ãƒ',
                             'd', 'Î´', 'ê¦´', 'ã‚©', 'Ã­', 'b', 'Å¯', 'Å¾', 'Ä ', 'á€„', 'áƒ', 'à¤œ', 'â¾', 'äº•',
                             'í•´', 'âˆ†', 'Î£', 'è»Š', 'ğ¤¯', 'É·', 'Ù', 'å‹', 'Ñ†', 'Øº', 'ä¸€', 'æ¡œ', 'åƒ',
                             'á€€', 'Ã‡', 'è¨', 'é©', 'à¤¶', 'á¼Œ', 'á¢', 'é¸', 'ãƒ­', 'Ù', 'ğ¤ ', 'ë§', 'Ä‡',
                             'çµµ', 'ì¡±', 'U', 'á¿¥', 'à´²', 'Ğ¸', 'à¸©', 'è¶³', 'É«', 'á¼´', 'Ï„', 'ãƒ´', 'ç¿¼',
                             'ì…', 'à´°', 'ã‚‚', 'ãƒœ', 'ã‚²', 't', 'à´¶', '×§', 'åœ˜', 'ãƒ€', 'å­¦', 'Ïƒ', 'Ğ†',
                             'Ğ¶', 'ÑŠ', 'æ–™', 'å£Œ', 'Ã¶', 'æ¸¯', 'é›»', 'Ã—', 'ê¦ƒ', 'å³', 'Ã¿', 'Ğ–', 'çš„',
                             'f', 'åŠ¨', 'Ä›', 'á½…', 'æ ¼', 'Â£', '×£', 'ì‹', 'D', 'å¯¿', 'Ğ˜', 'Ã“', 'á€¼',
                             'à¤š', 'ÅŸ', '×–', 'Ä…', 'ë°©', 'Ñƒ', 'Ã˜', 'à½‘', 'â„', 'ï¼µ', 'Â²', 'Ì‚', 'áƒ“', 'Ú¯',
                             'é’±', 'ë°˜', 'ã‚¤', 'à¹„', 'å¹´', 'âˆ—', 'É¯', 'Ö¥', 'É¦', 'Î©', 'â€', 'æ—', 'Ğ¢',
                             'å»', 'ã¡', 'æ™‚', 'Õ«', 'à¤­', 'Ã»', 'ãƒ—', 'æ–‡', 'F', 'ÛŒ', 'Ï•', 'Ğ´', 'ç·¨',
                             'ã‘', 'Îš', 'Ñ', 'Ğ·', '×—', 'æ¢', 'ç±³', 'ã ', 'Ã¡', 'Ã³', 'Ã½', 'à¸³', 'âˆ‚',
                             'æ­»', 'Ğ¾', 'à¸ˆ', 'éš›', 'à´¾', 'T', 'èª', 'Íœ', 'è§’', 'Ø¦', 'ã‚‹', 'è¿', 'å¤®',
                             'ë‹', 'Ç’', 'å™´', 'ãƒ‹', 'ê¦¶', 'áº–', 'Ñ‹', 'Õ¸', 'ê¦', 'É‘', 'à¤¯', 'Î†', 'Ø­', 'ç•¥',
                             'è§£', 'G', 'ä¸­', 'Ê', 'ãƒ', 'å¾·', 'ëŒ', 'W', 'åº§', 'Ñ»', 'áƒ¢', 'ì€', 'é›†',
                             '×¥', 'Ğ¡', 'å¾ˆ', 'ãƒ“', 'Â½', 'æ—…', 'ì§“', 'Ã‰', 'Ã±', 'ëŒ€', 'ãƒ', 'Û', 'çº§',
                             'Ï', 'Ğ£', 'â†’', 'á¿†', 'æµ', 'éŸ“', 'Ô¼', 'éŒ²', 'í•˜', 'Ã„', 'æ‰€', 'è²§', 'Å‘',
                             'ë°€', 'ãƒ¨', 'é¤Š', 'æ¦®', 'á€¬', 'å‰¿', 'à¼‹', 'à¤•', 'ç–¾', 'ìœ„', 'æ¢¯', 'á€',
                             'åŸ´', 'å¸', 'à¤–', 'æˆ¸', 'ëŸ°', 'á—', 'å´', 'áƒ', 'å‘', 'å—', 'ä¿¡', 'Ö²', 'Î˜',
                             'Î', 'ç¥', 'å·', 'Ü', 'å¾³', 'âˆ ', 'Õ¥', 'Üš', 'Æ', 'ÈŸ', 'å‘Š', 'ã‚¿', 'â˜ ',
                             'å€', 'è¶…', 'ç‚‰', 'ì„±', 'ã‚„', 'e', 'Åš', 'àµ‡', 'Ë‘', 'Ç', 'à¤¡', 'Ğ•', 'é“',
                             'æ•', 'Ëˆ', 'ì', 'á¹¬', 'Ö–', 'Ìª', 'Îµ', 'å¢ƒ', 'é€²', 'Ïˆ', 'é›„', 'å¥ª', 'ä¹',
                             'æ½˜', 'é¾', 'M', 'à¤µ', 'ë¡œ', 'áº¥', 'åˆ»', 'å½¼', 'Î”', 'ã‚¢', 'à¹‡', 'Ø±', 'áº§',
                             'J', 'å¿ƒ', 'â™­', 'Ö£', 'ã‚¼', 'ãƒ¬', 'Ñ£', 'ã‚«', 'ë‘', 'å', 'Ğš', 'æ‹', 'ê´´',
                             'Û', 'ã', 'ã‚¡', 'ç”²', 'Ø¬', 'N', 'à´¤', 'à¥©', 'Î‘', 'Ø¯', 'ê±°', '×¢', 'Ï‰',
                             'â€¿', 'ç”°', 'Ğ­', 'Ğ', 'à¸¢', 'áƒ”', 'é›¶', 'á ¶', 'åˆ', 'æ€€', 'Ü˜', 'Å›', 'Ö',
                             'à¦•', 'á“', 'Ø·', 'äº‹', 'ã¸', 'é›', 'å­', 'à²¦', 'Õ¦', 'ç‘¶', 'á ¡', 'Å«', 'á€¡',
                             'á½', 'Å±', 'ê¦¤', 'ç™½', 'à¤¹', 'Ñ˜', 'æ­¦', 'á¹…', 'Ã¹', 'ä»£', 'ì‚¬', 'ğ¤­', 'ç”µ',
                             'Ø¹', 'ç›¤', 'Ô±', 'ä¿', 'áŒ', 'à¤ƒ', 'Ñ', 'Î¿', 'áº£', 'æˆ‘', 'åœ¨', 'á¹½', 'à¤®',
                             'å²­', 'Ã–', 'Ãˆ', 'ç»¼', 'éƒ', 'z', 'æœ‰', 'ìŸ', 'l', 'X', 'ç¹', 'èˆ¹', 'ãƒƒ',
                             'ç§‘', 'áŸ', 'K', 'Ö»', 'ãƒ§', 'ç™¾', 'ã§', 'à¯', 'á¿–', 'Ğœ', 'áƒ‘', 'à¤¿', 'æ”¿',
                             'u', 'ÊŒ', 'Ê²', 'Ê', 'à¸—', 'Ô¿', 'æ—º', 'ã‚ª', 'y', 'æ­£', 'æ°¸', 'Îœ', 'ì²­',
                             'á¶', 'æ³', 'Ù„', 'Ù‰', 'á– ', '`', 'ãƒ•', 'ç ”', 'ä»™', 'íƒ•', 'å¼µ', 'Ä‹', 'é€š',
                             'Å“', 'Å', 'ë¦½', 'ä¸‹', 'ê¦­', 'ï¼¯', 'Ä¡', 'áŠ­', 'Ç', 'í‘œ', 'Ù±', 'á ­', 'Û',
                             'è¾±', 'à¦¶', 'è±¡', 'ç©¶', 'Ü£', 'à¤¥', 'v', 'á•¿', 'Ü’', 'å¥‹', 'é¾™', 'èµ·', 'q',
                             'æ±Ÿ', 'Î¼', 'ç™º', 'à¹Œ', 'é‡', 'Ğ®', 'è§‚', 'ì¡°', 'í–¥', 'é¢¨', 'è‚¡', 'æŒ‘', 'à¥',
                             'è§€', 'âˆ', 'ã•', 'ìš°', 'é£', 'Ö¾', 'ç”Ÿ', 'å¿—', 'æœ¬', 'á€¯', 'á´œ', 'à¤¤', 'Ê¼',
                             'Ï‡', 'á °', 'ì‚¼', 'è®º', 'çˆ›', 'ãƒ£', 'Êƒ', 'ÙŠ', 'áƒ«', 'Ğ¼', 'âŸ©', 'åš', 'è˜‡',
                             'Ï', 'ä¸ƒ', 'å‹™', 'ãƒ', 'C', '\xad', 'ê³ ', 'í’€', 'ã‚£', 'Â°', 'ã£', 'á›',
                             'ğ¤¦', 'ç‰¹', 'â²­', 'Ë', 'æœ›', 'á¼¡', 'ã°', 'ã“', 'ïº´', 'á»‘', 'é•‡', 'àµ', 'ã‚¥',
                             'è’¼', 'à¸š', 'è”˜', 'Ü›', 'Ğ‘', 'å½“', 'ã‚¨', 'å­”', 'Ö‘', 'ì •', 'áˆ', '\u200d',
                             'å¹³', 'á¼„', 'áƒ•', 'ì©Œ', 'Õ¬', 'Î“', 'Ö¿', 'Î¤', 'ì˜', 'ä¼Ÿ', 'áŠ–', 'Ü™', 'Ä—', 'æ›¾',
                             'ÏŒ', 'Ë©', 'à¸¥', 'è­‰', 'æ´ª', 'ãƒ©', 'áƒ ', 'ê¦º', 'ç ²', 'Ù’', 'Éª', 'Ğ³', 'Ã‘',
                             'Ï', 'Ì¯', 'á¸–', 'Ø®', 'è¼ª', 'à®¤', 'á  ', 'ë‹´', 'å˜', 'å®Ÿ', 'æ²–', '×¤', 'Ø¥',
                             'Î³', 'Ó™', 'â€˜', 'åŒº', 'ç©—', 'P', '\\', 'í˜¸', 'ç³»', '\u200e', 'á¹“', 'ç•°',
                             'ì™€', 'é€¸', 'è«–', 'æ°', 'S', 'ç¾', 'Ì½', 'Õµ', 'Ê°', 'åœ', 'à¸Š', 'à¤£', 'ãƒ‡',
                             'Ä°', 'Æ•', 'ĞŸ', 'ç«‹', 'í•¨', 'Î¨', 'o', 'ã‚°', 'æœ¨', 'Â¿', 'c', 'è€…', 'ãƒ',
                             'Ä§', 'å®¿', 'áº¯', 'à¤§', 'ä¸Š', 'ì—¬', 'å…´', 'ç—…', 'æ˜Ÿ', 'L', 'É’', 'Î™', 'è¨˜',
                             'á¿¦', 'Ä€', 'æ·¸', 'ÃŸ', 'ãƒ¢', 'é§…', 'ãª', 'â€•', 'á ¨', 'Ü•', 'Ü©', 'á¸¤', 'æ–¼',
                             'ãŒ', 'Ã¨', 'ÊŠ', 'åº•', 'É', '*', 'à¥‡', 'Ì', 'Ê€', 'ë¬´', 'å¼ ', 'â‰¡', 'Ö¶', 'åœ°',
                             'ì–´', 'ì‹ ', 'Ê', 'é€ ', 'êœœ', 'Ã¼', 'èƒ½', 'Ù', 'ã‚Š', 'å½±', 'å‘½', 'å ±', 'í¬',
                             'ï¼²', 'ì•Œ', 'á˜', 'Ğ—', 'É²', 'æ—¶', '×ƒ', 'é„­', 'à¤°', 'å…¬', 'é«”', 'å¸‚', 'é´¬',
                             'Ê¿', 'á¼¶', 'á€¦', 'ãƒ«', 'Ë—', 'áŸ’', 'Ù‡', 'm', 'á¼', 'é§†', 'çƒ', 'ç´”', 'Ö„',
                             'æ‰¶', 'è£', 'Ğ›', 'éº»', 'I', 'å‹', 'à¸‡', 'å·»', 'ãƒ›', 'æš®', 'ã„', 'å½°',
                             'æ‰', 'ê°€', 'Ã…', 'Ê•', 'à¤…', 'á€‚', 'à¥‹', 'Ú†', 'Ì„', 'áƒ—', 'Ï…'}

        Keep = {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',
                'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',
                '-', "'"}

        alphabetsToRemove = list(alphabetsToRemove.difference(Keep))

        for i in range(numArtices):
            newList = []
            for word in allArticles[i]:
                flag = 0
                for char in alphabetsToRemove:
                    if char in word:
                        flag = 1
                        break
                if flag == 0:
                    newList.append(word)
            allArticles[i] = newList


        s = set()
        numArtices = len(allArticles)
        for i in range(numArtices):
            for j in allArticles[i]:
                s.add(j)

        print("Vocab of all articles after preproccessing = ", len(s))

        with open('wikipediaArticlesProcessed.json', 'w') as outfile:
            json.dump(allArticles, outfile)

        '''

        # Load Queries
        queries_json = json.load(open(args.dataset + "cran_queries.json", 'r'))[:]

        query_ids, queries = [item["query number"] for item in queries_json], \
                             [item["query"] for item in queries_json]

        # Process queries
        processedQueries = self.preprocessQueries(queries, 'docs')

        # Load Cranfield Data
        docs_json = json.load(open(args.dataset + "cran_docs.json", 'r'))[:]
        title_ids, titles = [item["id"] for item in docs_json], \
                            [item["title"] for item in docs_json]

        # Process documents
        processedTitles = self.preprocessDocs(titles, 'docs')

        # Load all Preprocessed wikipedia articles
        allArticles = json.load(open("wikipediaArticlesProcessed.json", 'r'))[:5298]

        self.explicitSemanticAnalysis.buildIndexArticle(allArticles)

        self.explicitSemanticAnalysis.buildIndexConcept(processedTitles, title_ids, processedQueries)

        doc_IDs_ordered = self.explicitSemanticAnalysis.rank(processedTitles, processedQueries)

        # Read relevance judements
        qrels = json.load(open(args.dataset + "cran_qrels.json", 'r'))[:]

        # Calculate precision, recall, f-score, MAP and nDCG for k = 1 to 10
        precisions, recalls, fscores, MAPs, nDCGs = [], [], [], [], []
        for k in range(1, 11):
            precision = self.evaluator.meanPrecision(
                doc_IDs_ordered, query_ids, qrels, k)
            precisions.append(precision)

            recall = self.evaluator.meanRecall(
                doc_IDs_ordered, query_ids, qrels, k)
            recalls.append(recall)

            fscore = self.evaluator.meanFscore(
                doc_IDs_ordered, query_ids, qrels, k)
            fscores.append(fscore)

            print("Precision, Recall and F-score @ " +
                  str(k) + " : " + str(precision) + ", " + str(recall) +
                  ", " + str(fscore))
            MAP = self.evaluator.meanAveragePrecision(
                doc_IDs_ordered, query_ids, qrels, k)
            MAPs.append(MAP)
            nDCG = self.evaluator.meanNDCG(
                doc_IDs_ordered, query_ids, qrels, k)
            nDCGs.append(nDCG)
            print("MAP, nDCG @ " +
                  str(k) + " : " + str(MAP) + ", " + str(nDCG))

        # Plot the metrics and save plot
        plt.plot(range(1, 11), precisions, label="Precision")
        plt.plot(range(1, 11), recalls, label="Recall")
        plt.plot(range(1, 11), fscores, label="F-Score")
        plt.plot(range(1, 11), MAPs, label="MAP")
        plt.plot(range(1, 11), nDCGs, label="nDCG")
        plt.legend()
        plt.title("ESA - Cranfield Dataset")
        plt.xlabel("k")
        plt.savefig(args.out_folder + "eval_plot_ESA.png")


if __name__ == "__main__":

    # Create an argument parser
    parser = argparse.ArgumentParser(description='main.py')

    # Tunable parameters as external arguments
    parser.add_argument('-dataset', default="cranfield/",
                        help="Path to the dataset folder")
    parser.add_argument('-out_folder', default="output/",
                        help="Path to output folder")
    parser.add_argument('-segmenter', default="punkt",
                        help="Sentence Segmenter Type [naive|punkt]")
    parser.add_argument('-tokenizer', default="ptb",
                        help="Tokenizer Type [naive|ptb]")
    parser.add_argument('-custom', action="store_true",
                        help="Take custom query as input")

    # Parse the input arguments
    args = parser.parse_args()

    # Create an instance of the Search Engine
    searchEngine = SearchEngine(args)

    print("For running ESA type 0")
    print("For running LSA type 1")

    n = int(input())

    if n == 0:
        searchEngine.handleDatasetESA()
    else:
        searchEngine.handleDatasetLSA()